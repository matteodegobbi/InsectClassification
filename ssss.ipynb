{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b1bc88-56c2-4bff-b41e-63d6cb4b8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ops\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a15a0e-811f-42e1-a1be-bf11cbee23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0f1eb44-6b15-460b-8823-5b6e808fb6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2652910883.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[36], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    DATA:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "DATA:\n",
    "  name: \"CIFAR10\"\n",
    "  img_size: 32\n",
    "  num_classes: 10\n",
    "MODEL:\n",
    "  backbone: \"big_resnet\"\n",
    "  g_cond_mtd: \"cBN\"\n",
    "  d_cond_mtd: \"D2DCE\"\n",
    "  normalize_d_embed: True\n",
    "  d_embed_dim: 512\n",
    "  apply_g_sn: True\n",
    "  apply_d_sn: True\n",
    "  apply_attn: True\n",
    "  attn_g_loc: [2]\n",
    "  attn_d_loc: [1]\n",
    "  z_dim: 80\n",
    "  g_shared_dim: 128\n",
    "  g_conv_dim: 96\n",
    "  d_conv_dim: 96\n",
    "  apply_g_ema: True\n",
    "  g_ema_decay: 0.9999\n",
    "  g_ema_start: 1000\n",
    "LOSS:\n",
    "  adv_loss: \"hinge\"\n",
    "  cond_lambda: 0.5\n",
    "  m_p: 0.98\n",
    "  temperature: 0.5\n",
    "OPTIMIZATION:\n",
    "  batch_size: 128\n",
    "  g_lr: 0.00028284271\n",
    "  d_lr: 0.00028284271\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA:\n",
    "  name: \"ImageNet\"\n",
    "  img_size: 128\n",
    "  num_classes: 1000\n",
    "MODEL:\n",
    "  backbone: \"big_resnet_deep_legacy\"\n",
    "  g_cond_mtd: \"cBN\"\n",
    "  d_cond_mtd: \"D2DCE\"\n",
    "  normalize_d_embed: True\n",
    "  d_embed_dim: 2048\n",
    "  apply_g_sn: True\n",
    "  apply_d_sn: True\n",
    "  apply_attn: True\n",
    "  attn_g_loc: [4]\n",
    "  attn_d_loc: [1]\n",
    "  z_dim: 128\n",
    "  g_shared_dim: 128\n",
    "  g_conv_dim: 128\n",
    "  d_conv_dim: 128\n",
    "  g_depth: 2\n",
    "  d_depth: 2\n",
    "  apply_g_ema: True\n",
    "  g_ema_decay: 0.9999\n",
    "  g_ema_start: 20000\n",
    "LOSS:\n",
    "  adv_loss: \"hinge\"\n",
    "  cond_lambda: 0.5\n",
    "  m_p: 0.90\n",
    "  temperature: 0.25\n",
    "OPTIMIZATION:\n",
    "  batch_size: 256\n",
    "  acml_steps: 1\n",
    "  g_lr: 0.00005\n",
    "  d_lr: 0.0002\n",
    "  beta1: 0.0\n",
    "  beta2: 0.999\n",
    "  d_updates_per_step: 2\n",
    "  total_steps: 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59f2c08-d790-49dd-92f1-961ee5b2039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "def define_modules(gan_config):\n",
    "    layers = types.SimpleNamespace()\n",
    "    if gan_config.apply_g_sn:\n",
    "        layers.g_conv2d = ops.snconv2d\n",
    "        layers.g_deconv2d = ops.sndeconv2d\n",
    "        layers.g_linear = ops.snlinear\n",
    "        layers.g_embedding = ops.sn_embedding\n",
    "    else:\n",
    "        layers.g_conv2d = ops.conv2d\n",
    "        layers.g_deconv2d = ops.deconv2d\n",
    "        layers.g_linear = ops.linear\n",
    "        layers.g_embedding = ops.embedding\n",
    "\n",
    "    if gan_config.apply_d_sn:\n",
    "        layers.d_conv2d = ops.snconv2d\n",
    "        layers.d_deconv2d = ops.sndeconv2d\n",
    "        layers.d_linear = ops.snlinear\n",
    "        layers.d_embedding = ops.sn_embedding\n",
    "    else:\n",
    "        layers.d_conv2d = ops.conv2d\n",
    "        layers.d_deconv2d = ops.deconv2d\n",
    "        layers.d_linear = ops.linear\n",
    "        layers.d_embedding = ops.embedding\n",
    "\n",
    "    if gan_config.g_cond_mtd == \"cBN\":\n",
    "        layers.g_bn = ops.ConditionalBatchNorm2d\n",
    "    elif gan_config.g_cond_mtd == \"W/O\":\n",
    "        layers.g_bn = ops.batchnorm_2d\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if not gan_config.apply_d_sn:\n",
    "        layers.d_bn = ops.batchnorm_2d\n",
    "\n",
    "    if gan_config.g_act_fn == \"ReLU\":\n",
    "        layers.g_act_fn = nn.ReLU(inplace=True)\n",
    "    elif gan_config.g_act_fn == \"Leaky_ReLU\":\n",
    "        layers.g_act_fn = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "    elif gan_config.g_act_fn == \"ELU\":\n",
    "        layers.g_act_fn = nn.ELU(alpha=1.0, inplace=True)\n",
    "    elif gan_config.g_act_fn == \"GELU\":\n",
    "        layers.g_act_fn = nn.GELU()\n",
    "    elif gan_config.g_act_fn == \"Auto\":\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if gan_config.d_act_fn == \"ReLU\":\n",
    "        layers.d_act_fn = nn.ReLU(inplace=True)\n",
    "    elif gan_config.d_act_fn == \"Leaky_ReLU\":\n",
    "        layers.d_act_fn = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "    elif gan_config.d_act_fn == \"ELU\":\n",
    "        layers.d_act_fn = nn.ELU(alpha=1.0, inplace=True)\n",
    "    elif gan_config.d_act_fn == \"GELU\":\n",
    "        layers.d_act_fn = nn.GELU()\n",
    "    elif gan_config.g_act_fn == \"Auto\":\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f07cd9c-557e-4588-a950-3b060db25f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = types.SimpleNamespace()\n",
    "config.d_act_fn = \"ReLU\"\n",
    "config.g_act_fn = \"ReLU\"\n",
    "config.apply_d_sn= True\n",
    "config.apply_g_sn= True\n",
    "config.g_cond_mtd= \"cBN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268ed15f-5c1d-473f-8268-dba1e186865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=define_modules(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30ce3584-ca32-4408-947b-e45ee95d3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "719f82eb-0209-4cfc-934b-3d2168d7302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import GenBlock\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, g_shared_dim, img_size, g_conv_dim, apply_attn, attn_g_loc, g_cond_mtd, num_classes, g_init, g_depth,\n",
    "                 mixed_precision, MODULES, MODEL):\n",
    "        super(Generator, self).__init__()\n",
    "        g_in_dims_collection = {\n",
    "            \"32\": [g_conv_dim * 4, g_conv_dim * 4, g_conv_dim * 4],\n",
    "            \"64\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "            \"128\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "            \"256\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "            \"512\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim]\n",
    "        }\n",
    "\n",
    "        g_out_dims_collection = {\n",
    "            \"32\": [g_conv_dim * 4, g_conv_dim * 4, g_conv_dim * 4],\n",
    "            \"64\": [g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "            \"128\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "            \"256\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "            \"512\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim, g_conv_dim]\n",
    "        }\n",
    "\n",
    "        bottom_collection = {\"32\": 4, \"64\": 4, \"128\": 4, \"256\": 4, \"512\": 4}\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.g_cond_mtd = g_cond_mtd\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.MODEL = MODEL\n",
    "        self.in_dims = g_in_dims_collection[str(img_size)]\n",
    "        self.out_dims = g_out_dims_collection[str(img_size)]\n",
    "        self.bottom = bottom_collection[str(img_size)]\n",
    "        self.num_blocks = len(self.in_dims)\n",
    "        self.affine_input_dim = 0\n",
    "\n",
    "        info_dim = 0\n",
    "        if self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "            info_dim += self.MODEL.info_num_discrete_c*self.MODEL.info_dim_discrete_c\n",
    "        if self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "            info_dim += self.MODEL.info_num_conti_c\n",
    "\n",
    "        self.g_info_injection = self.MODEL.g_info_injection\n",
    "        if self.MODEL.info_type != \"N/A\":\n",
    "            if self.g_info_injection == \"concat\":\n",
    "                self.info_mix_linear = MODULES.g_linear(in_features=self.z_dim + info_dim, out_features=self.z_dim, bias=True)\n",
    "            elif self.g_info_injection == \"cBN\":\n",
    "                self.affine_input_dim += self.z_dim\n",
    "                self.info_proj_linear = MODULES.g_linear(in_features=info_dim, out_features=self.z_dim, bias=True)\n",
    "\n",
    "        self.linear0 = MODULES.g_linear(in_features=self.z_dim, out_features=self.in_dims[0] * self.bottom * self.bottom, bias=True)\n",
    "\n",
    "        if self.g_cond_mtd != \"W/O\" and self.g_cond_mtd == \"cBN\":\n",
    "            self.affine_input_dim += self.num_classes\n",
    "\n",
    "        self.blocks = []\n",
    "        for index in range(self.num_blocks):\n",
    "            self.blocks += [[\n",
    "                GenBlock(in_channels=self.in_dims[index],\n",
    "                         out_channels=self.out_dims[index],\n",
    "                         g_cond_mtd=self.g_cond_mtd,\n",
    "                         g_info_injection=self.g_info_injection,\n",
    "                         affine_input_dim=self.affine_input_dim,\n",
    "                         MODULES=MODULES)\n",
    "            ]]\n",
    "\n",
    "            if index + 1 in attn_g_loc and apply_attn:\n",
    "                self.blocks += [[ops.SelfAttention(self.out_dims[index], is_generator=True, MODULES=MODULES)]]\n",
    "\n",
    "        self.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "        self.bn4 = ops.batchnorm_2d(in_features=self.out_dims[-1])\n",
    "        self.activation = MODULES.g_act_fn\n",
    "        self.conv2d5 = MODULES.g_conv2d(in_channels=self.out_dims[-1], out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        ops.init_weights(self.modules, g_init)\n",
    "\n",
    "    def forward(self, z, label, shared_label=None, eval=False):\n",
    "        affine_list = []\n",
    "        if self.g_cond_mtd != \"W/O\":\n",
    "            label = F.one_hot(label, num_classes=self.num_classes).to(torch.float32)\n",
    "        with torch.cuda.amp.autocast() if self.mixed_precision and not eval else misc.dummy_context_mgr() as mp:\n",
    "            if self.MODEL.info_type != \"N/A\":\n",
    "                if self.g_info_injection == \"concat\":\n",
    "                    z = self.info_mix_linear(z)\n",
    "                elif self.g_info_injection == \"cBN\":\n",
    "                    z, z_info = z[:, :self.z_dim], z[:, self.z_dim:]\n",
    "                    affine_list.append(self.info_proj_linear(z_info))\n",
    "\n",
    "            if self.g_cond_mtd != \"W/O\":\n",
    "                affine_list.append(label)\n",
    "            if len(affine_list) > 0:\n",
    "                affines = torch.cat(affine_list, 1)\n",
    "            else:\n",
    "                affines = None\n",
    "\n",
    "            act = self.linear0(z)\n",
    "            act = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n",
    "            for index, blocklist in enumerate(self.blocks):\n",
    "                for block in blocklist:\n",
    "                    if isinstance(block, ops.SelfAttention):\n",
    "                        act = block(act)\n",
    "                    else:\n",
    "                        act = block(act, affines)\n",
    "\n",
    "            act = self.bn4(act)\n",
    "            act = self.activation(act)\n",
    "            act = self.conv2d5(act)\n",
    "            out = self.tanh(act)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d944cbe-3fe1-40fd-a05c-882c39459b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = types.SimpleNamespace()\n",
    "MODEL.info_type = 'N/A'\n",
    "MODEL.g_info_injection = 'N/A'\n",
    "#MODEL.info_num_discrete_c = 'discrete'\n",
    "#MODEL.info_dim_discrete_c = 'discrete'\n",
    "g = Generator(100,128,128,128,True,[4],\"cBN\",1000,g_init=\"ortho\",g_depth=2,mixed_precision=False,MODULES=l,MODEL=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "140010c9-e9ee-4364-8fe0-02529e535e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143197444\n"
     ]
    }
   ],
   "source": [
    "import dataset_utils\n",
    "n_params = dataset_utils.count_trainable_parameters(g);\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebc58364-ff79-48c5-988d-7e95e7b12f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size, d_conv_dim, apply_d_sn, apply_attn, attn_d_loc, d_cond_mtd, aux_cls_type, d_embed_dim, normalize_d_embed,\n",
    "                 num_classes, d_init, d_depth, mixed_precision, MODULES, MODEL):\n",
    "        super(Discriminator, self).__init__()\n",
    "        d_in_dims_collection = {\n",
    "            \"32\": [3] + [d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2],\n",
    "            \"64\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8],\n",
    "            \"128\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16],\n",
    "            \"256\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16],\n",
    "            \"512\": [3] + [d_conv_dim, d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16]\n",
    "        }\n",
    "\n",
    "        d_out_dims_collection = {\n",
    "            \"32\": [d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2],\n",
    "            \"64\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16],\n",
    "            \"128\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16],\n",
    "            \"256\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16],\n",
    "            \"512\":\n",
    "            [d_conv_dim, d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16]\n",
    "        }\n",
    "\n",
    "        d_down = {\n",
    "            \"32\": [True, True, False, False],\n",
    "            \"64\": [True, True, True, True, False],\n",
    "            \"128\": [True, True, True, True, True, False],\n",
    "            \"256\": [True, True, True, True, True, True, False],\n",
    "            \"512\": [True, True, True, True, True, True, True, False]\n",
    "        }\n",
    "\n",
    "        self.d_cond_mtd = d_cond_mtd\n",
    "        self.aux_cls_type = aux_cls_type\n",
    "        self.normalize_d_embed = normalize_d_embed\n",
    "        self.num_classes = num_classes\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.in_dims = d_in_dims_collection[str(img_size)]\n",
    "        self.out_dims = d_out_dims_collection[str(img_size)]\n",
    "        self.MODEL = MODEL\n",
    "        down = d_down[str(img_size)]\n",
    "\n",
    "        self.blocks = []\n",
    "        for index in range(len(self.in_dims)):\n",
    "            if index == 0:\n",
    "                self.blocks += [[\n",
    "                    DiscOptBlock(in_channels=self.in_dims[index], out_channels=self.out_dims[index], apply_d_sn=apply_d_sn, MODULES=MODULES)\n",
    "                ]]\n",
    "            else:\n",
    "                self.blocks += [[\n",
    "                    DiscBlock(in_channels=self.in_dims[index],\n",
    "                              out_channels=self.out_dims[index],\n",
    "                              apply_d_sn=apply_d_sn,\n",
    "                              MODULES=MODULES,\n",
    "                              downsample=down[index])\n",
    "                ]]\n",
    "\n",
    "            if index + 1 in attn_d_loc and apply_attn:\n",
    "                self.blocks += [[ops.SelfAttention(self.out_dims[index], is_generator=False, MODULES=MODULES)]]\n",
    "\n",
    "        self.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "        self.activation = MODULES.d_act_fn\n",
    "\n",
    "        # linear layer for adversarial training\n",
    "        if self.d_cond_mtd == \"MH\":\n",
    "            self.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=1 + num_classes, bias=True)\n",
    "        elif self.d_cond_mtd == \"MD\":\n",
    "            self.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            self.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=1, bias=True)\n",
    "\n",
    "        # double num_classes for Auxiliary Discriminative Classifier\n",
    "        if self.aux_cls_type == \"ADC\":\n",
    "            num_classes = num_classes * 2\n",
    "\n",
    "        # linear and embedding layers for discriminator conditioning\n",
    "        if self.d_cond_mtd == \"AC\":\n",
    "            self.linear2 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=False)\n",
    "        elif self.d_cond_mtd == \"PD\":\n",
    "            self.embedding = MODULES.d_embedding(num_classes, self.out_dims[-1])\n",
    "        elif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "            self.linear2 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=d_embed_dim, bias=True)\n",
    "            self.embedding = MODULES.d_embedding(num_classes, d_embed_dim)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # linear and embedding layers for evolved classifier-based GAN\n",
    "        if self.aux_cls_type == \"TAC\":\n",
    "            if self.d_cond_mtd == \"AC\":\n",
    "                self.linear_mi = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=False)\n",
    "            elif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "                self.linear_mi = MODULES.d_linear(in_features=self.out_dims[-1], out_features=d_embed_dim, bias=True)\n",
    "                self.embedding_mi = MODULES.d_embedding(num_classes, d_embed_dim)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        # Q head network for infoGAN\n",
    "        if self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "            out_features = self.MODEL.info_num_discrete_c*self.MODEL.info_dim_discrete_c\n",
    "            self.info_discrete_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "        if self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "            out_features = self.MODEL.info_num_conti_c\n",
    "            self.info_conti_mu_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "            self.info_conti_var_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "\n",
    "        if d_init:\n",
    "            ops.init_weights(self.modules, d_init)\n",
    "\n",
    "    def forward(self, x, label, eval=False, adc_fake=False):\n",
    "        with torch.cuda.amp.autocast() if self.mixed_precision and not eval else misc.dummy_context_mgr() as mp:\n",
    "            embed, proxy, cls_output = None, None, None\n",
    "            mi_embed, mi_proxy, mi_cls_output = None, None, None\n",
    "            info_discrete_c_logits, info_conti_mu, info_conti_var = None, None, None\n",
    "            h = x\n",
    "            for index, blocklist in enumerate(self.blocks):\n",
    "                for block in blocklist:\n",
    "                    h = block(h)\n",
    "            bottom_h, bottom_w = h.shape[2], h.shape[3]\n",
    "            h = self.activation(h)\n",
    "            h = torch.sum(h, dim=[2, 3])\n",
    "\n",
    "            # adversarial training\n",
    "            adv_output = torch.squeeze(self.linear1(h))\n",
    "\n",
    "            # make class labels odd (for fake) or even (for real) for ADC\n",
    "            if self.aux_cls_type == \"ADC\":\n",
    "                if adc_fake:\n",
    "                    label = label*2 + 1\n",
    "                else:\n",
    "                    label = label*2\n",
    "\n",
    "            # forward pass through InfoGAN Q head\n",
    "            if self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "                info_discrete_c_logits = self.info_discrete_linear(h/(bottom_h*bottom_w))\n",
    "            if self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "                info_conti_mu = self.info_conti_mu_linear(h/(bottom_h*bottom_w))\n",
    "                info_conti_var = torch.exp(self.info_conti_var_linear(h/(bottom_h*bottom_w)))\n",
    "\n",
    "            # class conditioning\n",
    "            if self.d_cond_mtd == \"AC\":\n",
    "                if self.normalize_d_embed:\n",
    "                    for W in self.linear2.parameters():\n",
    "                        W = F.normalize(W, dim=1)\n",
    "                    h = F.normalize(h, dim=1)\n",
    "                cls_output = self.linear2(h)\n",
    "            elif self.d_cond_mtd == \"PD\":\n",
    "                adv_output = adv_output + torch.sum(torch.mul(self.embedding(label), h), 1)\n",
    "            elif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "                embed = self.linear2(h)\n",
    "                proxy = self.embedding(label)\n",
    "                if self.normalize_d_embed:\n",
    "                    embed = F.normalize(embed, dim=1)\n",
    "                    proxy = F.normalize(proxy, dim=1)\n",
    "            elif self.d_cond_mtd == \"MD\":\n",
    "                idx = torch.LongTensor(range(label.size(0))).to(label.device)\n",
    "                adv_output = adv_output[idx, label]\n",
    "            elif self.d_cond_mtd in [\"W/O\", \"MH\"]:\n",
    "                pass\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            # extra conditioning for TACGAN and ADCGAN\n",
    "            if self.aux_cls_type == \"TAC\":\n",
    "                if self.d_cond_mtd == \"AC\":\n",
    "                    if self.normalize_d_embed:\n",
    "                        for W in self.linear_mi.parameters():\n",
    "                            W = F.normalize(W, dim=1)\n",
    "                    mi_cls_output = self.linear_mi(h)\n",
    "                elif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "                    mi_embed = self.linear_mi(h)\n",
    "                    mi_proxy = self.embedding_mi(label)\n",
    "                    if self.normalize_d_embed:\n",
    "                        mi_embed = F.normalize(mi_embed, dim=1)\n",
    "                        mi_proxy = F.normalize(mi_proxy, dim=1)\n",
    "        return {\n",
    "            \"h\": h,\n",
    "            \"adv_output\": adv_output,\n",
    "            \"embed\": embed,\n",
    "            \"proxy\": proxy,\n",
    "            \"cls_output\": cls_output,\n",
    "            \"label\": label,\n",
    "            \"mi_embed\": mi_embed,\n",
    "            \"mi_proxy\": mi_proxy,\n",
    "            \"mi_cls_output\": mi_cls_output,\n",
    "            \"info_discrete_c_logits\": info_discrete_c_logits,\n",
    "            \"info_conti_mu\": info_conti_mu,\n",
    "            \"info_conti_var\": info_conti_var\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "182f8699-9210-4084-b8fd-b7542d401d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import DiscBlock, DiscOptBlock\n",
    "d = Discriminator(128,128,True,True,[1],\"D2DCE\",\"W/O\",2048,True,1000,\"ortho\",2,False,MODULES=l,MODEL=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcafd482-2c86-4a7d-a936-1ccf2af8e6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159919490\n"
     ]
    }
   ],
   "source": [
    "n_params = dataset_utils.count_trainable_parameters(d);\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69c1b358-c880-4110-8e3c-37c40752b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g.cuda()\n",
    "d= d.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06084fd4-5158-47a8-bd81-429438dea787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba0adc60-f5fe-4c72-a0cb-f55878959943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "df = pd.read_csv('final_dataset.csv',index_col=0)\n",
    "tform = transforms.Compose([transforms.Resize((64,64)),transforms.PILToTensor(),transforms.ConvertImageDtype(torch.float),transforms.Normalize(0.5,0.5)])\n",
    "image_dataset = torchvision.datasets.ImageFolder(\"image_dataset/\",transform=tform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e6a3487-eef1-4639-a9e2-d42ce3456174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365    Bembidion normannum\n",
      "292       Bledius gallicus\n",
      "321       Praxis edwardsii\n",
      "352        Andrena pilipes\n",
      "18     Automeris managuana\n",
      "              ...         \n",
      "412         Hemiceras losa\n",
      "413         Hemiceras losa\n",
      "417     Hemiceras punctata\n",
      "418         Hemiceras losa\n",
      "421     Hemiceras punctata\n",
      "Name: species_name, Length: 9991, dtype: object\n",
      "32424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32424"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import dataset_utils\n",
    "import numpy as np\n",
    "img2dna = dataset_utils.get_imgs_bold_id(image_dataset,df)\n",
    "\n",
    "nucleotides = df[['nucleotide','species_name','genus_name','processid','image_urls']]\n",
    "colonna_dna = df.loc[:,\"nucleotide\"]\n",
    "nucleotides.loc[:,'nucleotide'] = colonna_dna.apply(dataset_utils.one_hot_encoding)\n",
    "random.seed(42)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = dataset_utils.data_split(nucleotides,0.2,random_state=42)\n",
    "print(y_test)\n",
    "train_data = X_train_val\n",
    "train_data['species_name'] = y_train_val\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = dataset_utils.data_split(train_data,0.2,drop_labels=False,random_state=42)\n",
    "train_indices, val_indices, test_indices = dataset_utils.image_splits_from_df(X_train,X_validation,X_test,image_dataset)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class WholeDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = data.targets#torch.tensor(targets)\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index][0]\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "whole_dataset = WholeDataset(image_dataset)\n",
    "n_classes = np.unique(whole_dataset.targets).shape[0]\n",
    "print(len(test_indices+val_indices+train_indices))\n",
    "len(whole_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1dd26b16-2277-47a2-a8e0-29497f18a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_size = 100 \n",
    "batch_size =  64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f99a25c-ca81-444b-8a0d-a85990943629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13039\n",
      "6955\n",
      "12430\n"
     ]
    }
   ],
   "source": [
    "train_imgs = torch.utils.data.Subset(whole_dataset, train_indices)\n",
    "val_imgs = torch.utils.data.Subset(whole_dataset, val_indices)\n",
    "test_imgs = torch.utils.data.Subset(whole_dataset, test_indices)\n",
    "len(train_imgs)+len(val_imgs)+len(test_imgs)\n",
    "train_loader = torch.utils.data.DataLoader(train_imgs, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_imgs, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_imgs, batch_size=4,shuffle=True, num_workers=2)\n",
    "del test_loader\n",
    "dataloaders = {\"train\":train_loader,\"val\":val_loader}\n",
    "dataset_sizes = {'train': len(train_imgs.indices), 'val':len(val_imgs.indices)}\n",
    "\n",
    "print(len(train_imgs.indices))\n",
    "print(len(val_imgs.indices))\n",
    "print(len(test_imgs.indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da37c522-7153-4bdf-a94c-3bb4b8bd6542",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m real_images, real_classes \u001b[38;5;129;01min\u001b[39;00m tqdm(fit_p\u001b[38;5;241m.\u001b[39mdataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[1;32m      4\u001b[0m             real_images \u001b[38;5;241m=\u001b[39m real_images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "        for real_images, real_classes in tqdm(fit_p.dataloaders['train']):\n",
    "            # Train discriminator\n",
    "            real_images = real_images.to(device)\n",
    "            real_classes = real_classes.to(device)\n",
    "            loss_d, real_score, fake_score, class_accuracy_real, class_accuracy_fake = train_discriminator(real_images, real_classes,fit_p.discriminator_optimizer,fit_p.discriminator,fit_p.generator,fit_p.batch_size,fit_p.latent_size,fit_p.described_species_labels,fit_p.device,fit_p.n_classes)\n",
    "            loss_g = train_generator(fit_p.generator_optimizer,fit_p.generator,fit_p.discriminator,fit_p.batch_size,fit_p.latent_size,fit_p.described_species_labels,fit_p.device,fit_p.n_classes)\n",
    "            \n",
    "        #save_samples(epoch+start_idx, save_p,fit_p.generator,fit_p.writer,show=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff1b95a1-86bd-4bcf-9d38-d6ac3c021b77",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m misc\n\u001b[0;32m----> 2\u001b[0m g(torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m54\u001b[39m,\u001b[38;5;241m10\u001b[39m),torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/insetti/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/insetti/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 92\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, z, label, shared_label, eval)\u001b[0m\n\u001b[1;32m     90\u001b[0m     affine_list\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(affine_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     affines \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(affine_list, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     affines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "from utils import misc\n",
    "g(torch.ones(54,10),torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182dedd3-d8bc-4a63-9d33-bd64e06d9a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

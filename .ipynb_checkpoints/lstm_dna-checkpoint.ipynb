{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151e3535-2b5f-4a7a-bebf-62eb92e2cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn \n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image \n",
    "import numpy as np\n",
    "import random\n",
    "import dataset_utils\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56710acb-c98c-4bee-80fa-c22212ea2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_dataset.csv',index_col=0)\n",
    "tform = transforms.Compose([transforms.Resize((64,64)),transforms.PILToTensor(),transforms.ConvertImageDtype(torch.float),transforms.Normalize(0.5,0.5)])\n",
    "image_dataset = torchvision.datasets.ImageFolder(\"image_dataset/\",transform=tform)\n",
    "species2genus = dataset_utils.species_label_to_genus_label(df,image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75578b68-28d6-4e79-80d4-2a2d653fa593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365    Bembidion normannum\n",
      "292       Bledius gallicus\n",
      "321       Praxis edwardsii\n",
      "352        Andrena pilipes\n",
      "18     Automeris managuana\n",
      "              ...         \n",
      "412         Hemiceras losa\n",
      "413         Hemiceras losa\n",
      "417     Hemiceras punctata\n",
      "418         Hemiceras losa\n",
      "421     Hemiceras punctata\n",
      "Name: species_name, Length: 9991, dtype: object\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000 \n",
    "import random\n",
    "import dataset_utils\n",
    "img2dna = dataset_utils.get_imgs_bold_id(image_dataset,df)\n",
    "\n",
    "nucleotides = df[['nucleotide','species_name','genus_name','processid','image_urls']]\n",
    "colonna_dna = df.loc[:,\"nucleotide\"]\n",
    "nucleotides.loc[:,'nucleotide'] = colonna_dna.apply(dataset_utils.one_hot_encoding)\n",
    "random.seed(42)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = dataset_utils.data_split(nucleotides,0.2,random_state=42)\n",
    "print(y_test)\n",
    "train_data = X_train_val\n",
    "train_data['species_name'] = y_train_val\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = dataset_utils.data_split(train_data,0.2,drop_labels=False,random_state=42)\n",
    "train_indices, val_indices, test_indices = dataset_utils.image_splits_from_df(X_train,X_validation,X_test,image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb68dd0-65f6-4879-bb32-4d4b2d9670be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(image_dataset.imgs)[train_indices][:,1].astype(int)\n",
    "val_labels = np.array(image_dataset.imgs)[val_indices][:,1].astype(int)\n",
    "\n",
    "y_train = y_train.apply(lambda x: image_dataset.class_to_idx[x.replace(' ','_')])\n",
    "y_test = y_test.apply(lambda x: image_dataset.class_to_idx[x.replace(' ','_')])\n",
    "y_validation= y_validation.apply(lambda x: image_dataset.class_to_idx[x.replace(' ','_')])\n",
    "y_train_val = y_train_val.apply(lambda x: image_dataset.class_to_idx[x.replace(' ','_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f4a4a7-694d-4a19-94bc-bf31ee866193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNAdataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.tensor(targets)\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(np.float32(self.data[index][0])).unsqueeze(0)\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        #if self.transform:\n",
    "        #    x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "        #    x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "d_train = DNAdataset(X_train.values, y_train.values)\n",
    "d_val = DNAdataset(X_validation.values, y_validation.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9cb6ac6-0340-43ea-b6ae-849cda31378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(d_train, batch_size=32,shuffle=True)\n",
    "dataloader_val = DataLoader(d_val, batch_size=32,shuffle=True)\n",
    "dataloaders = {'train':dataloader_train,'val':dataloader_val}\n",
    "dataset_sizes = {'train': d_train.data.shape[0], 'val':d_val.data.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3782733f-f6c1-40cc-9894-ba233f9f1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def fit(epochs,dataloaders,optimizer,model,start_idx=0):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_losses = []\n",
    "    train_scores = []\n",
    "    val_losses = []\n",
    "    val_scores = []\n",
    "    for epoch in range(epochs):\n",
    "        running_train_corrects = 0\n",
    "        for dnas,labels in tqdm(dataloaders['train']):\n",
    "            model.train()\n",
    "            dnas = dnas.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predicted_labels = model(dnas)\n",
    "            train_loss = criterion(predicted_labels,labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(predicted_labels, 1)\n",
    "            #print(preds)\n",
    "            #print(labels.data)\n",
    "            running_train_corrects += torch.sum(preds == labels.data)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        running_val_corrects = 0\n",
    "        for dnas,labels in tqdm(dataloaders['val']):\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dnas = dnas.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                predicted_labels = model(dnas)\n",
    "                val_loss = criterion(predicted_labels,labels)\n",
    "                \n",
    "                _, preds = torch.max(predicted_labels, 1)\n",
    "                #print(preds)\n",
    "                #print(labels.data)\n",
    "                running_val_corrects += torch.sum(preds == labels.data)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #real_scores.append(real_score)\n",
    "        #fit_p.writer.add_scalar('loss_g', loss_g, epoch)\n",
    "        # Log losses & scores (last batch)\n",
    "        \n",
    "        epoch_train_acc = running_train_corrects.double() / dataset_sizes['train']\n",
    "        epoch_val_acc = running_val_corrects.double() / dataset_sizes['val']\n",
    "        print(\"Epoch [{}/{}], train_loss: {:.4f},  train_score: {:.4f},val_loss: {:.4f},  val_score: {:.4f}\".format(\n",
    "            epoch+1, epochs, train_loss, epoch_train_acc,val_loss,epoch_val_acc))\n",
    "        #print(f\"class accuracy real {class_accuracy_real}\")\n",
    "    \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12811088-6083-4d28-9351-df69ae95fe12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CNN + LSTM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee95af2-1b9c-445b-90b2-9985e93f9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_CNN_LSTM1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Hybrid_CNN_LSTM, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, (5, 1))\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.norm1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 1, (5, 1))\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.norm2 = nn.BatchNorm2d(1)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(input_size=3250, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        #self.norm3 = nn.BatchNorm1d(128)\n",
    "        self.linear = nn.Linear(128, 1500)\n",
    "        self.dropout1 = nn.Dropout(0.70)\n",
    "        self.dropout2 = nn.Dropout(0.70)\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(1500, 1050)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        x = x.view(x.size(0), 1, -1) \n",
    "        \n",
    "        x, (hn, cn) = self.lstm(x)  \n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        #x = self.norm3(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def feature_extract(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        x = x.view(x.size(0), 1, -1) \n",
    "        \n",
    "        x, (hn, cn) = self.lstm(x)  \n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        #x = self.norm3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00febc98-9ebb-4cb9-9679-5a4ea3a421fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Hybrid_CNN_LSTM, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, (5, 1))\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.norm1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 1, (5, 1))\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.norm2 = nn.BatchNorm2d(1)\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # Adjust LSTM hidden size and add more layers\n",
    "        self.lstm = nn.LSTM(input_size=3250, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.norm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.linear1 = nn.Linear(128, 1500)\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear2 = nn.Linear(1500, 1050)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        \n",
    "        # Reshape for LSTM: (batch_size, seq_length, input_size)\n",
    "        x = x.view(x.size(0), 1, -1)  # Adding a sequence length dimension of 1\n",
    "        \n",
    "        x, (hn, cn) = self.lstm(x)  # LSTM output\n",
    "        \n",
    "        # Take the last output of the LSTM (if sequence length > 1, we take the last timestep)\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        x = self.norm3(x)  # Batch normalization after LSTM\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def feature_extract(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        \n",
    "        # Reshape for LSTM: (batch_size, seq_length, input_size)\n",
    "        x = x.view(x.size(0), 1, -1)  # Adding a sequence length dimension of 1\n",
    "        \n",
    "        x, (hn, cn) = self.lstm(x)  # LSTM output\n",
    "        \n",
    "        # Take the last output of the LSTM (if sequence length > 1, we take the last timestep)\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        x = self.norm3(x)  # Batch normalization after LSTM\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ccc714-e8bf-4f2f-a295-bf545a97ef31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ce3e5ed17e4007a3539b0dfb7dc2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805dfaef9d94479e88130521fb650277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], train_loss: 1.5401,  train_score: 0.6321,val_loss: 4.5138,  val_score: 0.4501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65ac12b84bc4dc4b2a14ca963e40607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29075ad1f7c4d549a465d24ea664749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], train_loss: 0.7681,  train_score: 0.9546,val_loss: 4.7134,  val_score: 0.4620\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8a2930e32342a0a5ff28d1e346b6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d001ea022b4e2399fded3db33cf16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], train_loss: 0.0070,  train_score: 0.9706,val_loss: 5.8789,  val_score: 0.4666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83816df2a169483b941d7d38640276d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e968aae829d439aaeabc9bc075380fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], train_loss: 0.0184,  train_score: 0.9782,val_loss: 8.7362,  val_score: 0.4647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72b641ecc5c44f5898b3f57f65b10c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c40cc759dc4c8ba10c65ab446996df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], train_loss: 0.0459,  train_score: 0.9803,val_loss: 5.3100,  val_score: 0.4657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8a79e12bf742ddb303dd6a508e7b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84670174876d42f9b3f03b459c24b6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], train_loss: 0.0121,  train_score: 0.9837,val_loss: 3.8422,  val_score: 0.4675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1e52dedd1845db8b42366dfd855d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531efdf58aee494fbb907ba271a4b2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], train_loss: 0.0816,  train_score: 0.9883,val_loss: 4.1957,  val_score: 0.4666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41d08e7b8af4be8bebce2b998f5060a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd09e27e8354fdc8618a7f556d55b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], train_loss: 0.0841,  train_score: 0.9842,val_loss: 8.3077,  val_score: 0.4693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612e64b8b86443aeb9557f3358f00b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8beaa5cd7f43bc896d78120f1282a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], train_loss: 0.0480,  train_score: 0.9896,val_loss: 5.4624,  val_score: 0.4678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dde5ea8d54428f9c2df63bd3432839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936f102f90184fbc9385f57c319d1e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], train_loss: 0.0033,  train_score: 0.9899,val_loss: 3.5494,  val_score: 0.4671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfe290a4425436e8d76db989f529cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5f36497a3440f8b56fff5eb9c149c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], train_loss: 0.2279,  train_score: 0.9876,val_loss: 4.8175,  val_score: 0.4671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7041b3f05b0441793eff9a3f4736dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec1dc71b3434daf98d2615f26fd876a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], train_loss: 0.0025,  train_score: 0.9925,val_loss: 4.4270,  val_score: 0.4702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0ffe07121b4c9ba1e587583f289657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b671250ee644f4935bc3a340b13086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], train_loss: 0.0049,  train_score: 0.9926,val_loss: 3.9473,  val_score: 0.4703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e5024ab16d4f2ba7307cbf4bf76db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374dac9efe8e465e86327c3d04f99c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], train_loss: 0.0028,  train_score: 0.9939,val_loss: 4.1694,  val_score: 0.4693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05de44c4b654751a95e61e62c2a75bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ec3940714e4198baefc9ce6f65a8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], train_loss: 0.0010,  train_score: 0.9934,val_loss: 4.8273,  val_score: 0.4705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723a99298f0b408e8a94326793c10626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701d8beee26a4cde840d45a1748049e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], train_loss: 0.0055,  train_score: 0.9964,val_loss: 2.9646,  val_score: 0.4702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e3f07161624284972d01444dc8cbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7f6357370c413f80e74fcc067adda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], train_loss: 0.0724,  train_score: 0.9911,val_loss: 4.3626,  val_score: 0.4691\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32e4e1b3a884e1fbc9195d3ec43d646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd534fb1ce4d434c8421b35f9a7d8dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], train_loss: 0.0101,  train_score: 0.9953,val_loss: 4.9601,  val_score: 0.4700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e871d58c47e949b297251dc740bd7184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8eb9c3dae44ec88f0723391aa25ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], train_loss: 0.0306,  train_score: 0.9945,val_loss: 5.1984,  val_score: 0.4686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8b2c2bd3c446539f1a01765b5776ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d59025e4a54f87adfb79ac89ffcdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], train_loss: 0.0011,  train_score: 0.9952,val_loss: 6.6171,  val_score: 0.4700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3863ac7e29e04eb0b1b70ff7b8f7c751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f7d600777545ef9b3d834084fc34d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], train_loss: 0.0021,  train_score: 0.9949,val_loss: 4.9315,  val_score: 0.4707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ea4ae454f644f98c152a7e812b0d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e37566f80be4258ada2fb9a7bdc8a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], train_loss: 0.0003,  train_score: 0.9984,val_loss: 5.8819,  val_score: 0.4702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dba49be3a947a68ac7cd890866267e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45add73a89f84f04bd78e0f20989368f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], train_loss: 0.0079,  train_score: 0.9977,val_loss: 4.1485,  val_score: 0.4705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c4473bd1d64dc2b46c584674cf91a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310c842283064fc89f750031b73c09d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], train_loss: 0.0008,  train_score: 0.9983,val_loss: 4.6194,  val_score: 0.4703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55120421be13450885d3ef652a8cb436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14081f578e8146a2bbc273216bfa3a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], train_loss: 0.0192,  train_score: 0.9958,val_loss: 3.5610,  val_score: 0.4696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(1.5401, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.7681, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0070, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0184, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0459, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0121, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0816, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0841, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0480, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0033, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.2279, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0025, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0049, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0028, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0010, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0055, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0724, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0101, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0306, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0011, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0021, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0003, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0079, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0008, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0192, grad_fn=<NllLossBackward0>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 658\n",
    "num_features = 5\n",
    "num_classes = 1050\n",
    "\n",
    "hybridmodel = Hybrid_CNN_LSTM()\n",
    "\n",
    "optimizer = torch.optim.Adam(hybridmodel.parameters(),weight_decay=1e-5)\n",
    "\n",
    "fit(25,dataloaders,optimizer,hybridmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c442cf09-9194-41c5-baaf-ebb62aad4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(d_train, batch_size=len(d_train),shuffle=False)\n",
    "dataloader_val = DataLoader(d_val, batch_size=len(d_val),shuffle=False)\n",
    "dataloaders = {'train':dataloader_train,'val':dataloader_val}\n",
    "dataset_sizes = {'train': d_train.data.shape[0], 'val':d_val.data.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33eaec2f-a449-4408-950d-bde8be74c5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10840, 128])\n"
     ]
    }
   ],
   "source": [
    "hybridmodel.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "\n",
    "    for dnas,labels in dataloaders['train']:\n",
    "        dnas = dnas.to(device)\n",
    "        train_dna_features = hybridmodel.feature_extract(dnas)\n",
    "        train_dna_labels = labels\n",
    "    for dnas,labels in dataloaders['val']:\n",
    "        dnas = dnas.to(device)\n",
    "        val_dna_features = hybridmodel.feature_extract(dnas)\n",
    "        val_dna_labels = labels\n",
    "print(train_dna_features.shape)\n",
    "\n",
    "train_dna_features = train_dna_features.cpu()\n",
    "val_dna_features = val_dna_features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6c66f7-1c36-4de5-8143-dfe9a5786481",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2dna_indices = dict()\n",
    "for k,v in img2dna.items():\n",
    "    dna_index = np.where(X_train['processid'].values == v)\n",
    "    if dna_index[0].size > 0:\n",
    "        dna_index = dna_index[0][0]\n",
    "        \n",
    "        for i,(name,_) in enumerate(image_dataset.imgs):\n",
    "            if name == k:\n",
    "                image_index = i\n",
    "                break\n",
    "        img2dna_indices[image_index] = dna_index\n",
    "    else:\n",
    "        dna_index = np.where(X_validation['processid'].values == v)\n",
    "        if dna_index[0].size > 0:\n",
    "            dna_index = dna_index[0][0]\n",
    "            for i,(name,_) in enumerate(image_dataset.imgs):\n",
    "                if name == k:\n",
    "                    image_index = i\n",
    "                    break\n",
    "            img2dna_indices[image_index] = dna_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b32ca09-38a9-4716-a9f2-93e082e35abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2dna_indices\n",
    "train_indices\n",
    "\n",
    "dna_features2 = []\n",
    "dna_labels2 = []\n",
    "for i in train_indices:\n",
    "    dna_features2.append(train_dna_features[img2dna_indices[i]])\n",
    "    dna_labels2.append(train_dna_labels[img2dna_indices[i]])\n",
    "expanded_train_dna_features = torch.stack(dna_features2)\n",
    "expanded_train_dna_labels = torch.stack(dna_labels2)\n",
    "\n",
    "dna_features2 = []\n",
    "dna_labels2 = []\n",
    "for i in val_indices:\n",
    "    dna_features2.append(val_dna_features[img2dna_indices[i]])\n",
    "    dna_labels2.append(val_dna_labels[img2dna_indices[i]])\n",
    "expanded_val_dna_features = torch.stack(dna_features2)\n",
    "expanded_val_dna_labels = torch.stack(dna_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417394c9-2289-44cf-b7cf-9150fce2e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(expanded_train_dna_features,'features/lstm_expanded_train_dna_features.pt')\n",
    "torch.save(expanded_val_dna_features,'features/lstm_expanded_val_dna_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86ef4c82-e011-4dcb-883e-f8284d2471f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(expanded_train_dna_labels,'features/lstm_expanded_train_dna_labels.pt')\n",
    "torch.save(expanded_val_dna_labels,'features/lstm_expanded_val_dna_labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc80c70-7cb5-4e29-baec-3f3554922677",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Deeper CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f95a1ca5-485c-400d-bcbe-2cc68036b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeeperCNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, (5, 1))\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, (5, 1))\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 32, (5, 1))\n",
    "        self.activation3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(32, 64, (5, 1))\n",
    "        self.activation4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d((2, 1))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(11840, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.activation5 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.activation6 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 1050)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.activation5(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.activation6(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def feature_extract(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762be00-0c49-49b7-b5bb-5ecf948839f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepermodel = DeeperCNNModel()\n",
    " \n",
    "optimizer = torch.optim.Adam(deepermodel.parameters(),weight_decay=1e-5)\n",
    "\n",
    "fit(60,dataloaders,optimizer,deepermodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "097af246-67ac-40f4-b213-bcd645c33b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(d_train, batch_size=len(d_train),shuffle=False)\n",
    "dataloader_val = DataLoader(d_val, batch_size=len(d_val),shuffle=False)\n",
    "dataloaders = {'train':dataloader_train,'val':dataloader_val}\n",
    "dataset_sizes = {'train': d_train.data.shape[0], 'val':d_val.data.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5477dd5a-c78e-4a14-91bd-f6281ca53ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10840, 11840])\n"
     ]
    }
   ],
   "source": [
    "deepermodel.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "\n",
    "    for dnas,labels in dataloaders['train']:\n",
    "        dnas = dnas.to(device)\n",
    "        train_dna_features = deepermodel.feature_extract(dnas)\n",
    "        train_dna_labels = labels\n",
    "    for dnas,labels in dataloaders['val']:\n",
    "        dnas = dnas.to(device)\n",
    "        val_dna_features = deepermodel.feature_extract(dnas)\n",
    "        val_dna_labels = labels\n",
    "print(train_dna_features.shape)\n",
    "\n",
    "train_dna_features = train_dna_features.cpu()\n",
    "val_dna_features = val_dna_features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a02692b-236d-4452-9d20-4067ee34260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2dna_indices = dict()\n",
    "for k,v in img2dna.items():\n",
    "    dna_index = np.where(X_train['processid'].values == v)\n",
    "    if dna_index[0].size > 0:\n",
    "        dna_index = dna_index[0][0]\n",
    "        \n",
    "        for i,(name,_) in enumerate(image_dataset.imgs):\n",
    "            if name == k:\n",
    "                image_index = i\n",
    "                break\n",
    "        img2dna_indices[image_index] = dna_index\n",
    "    else:\n",
    "        dna_index = np.where(X_validation['processid'].values == v)\n",
    "        if dna_index[0].size > 0:\n",
    "            dna_index = dna_index[0][0]\n",
    "            for i,(name,_) in enumerate(image_dataset.imgs):\n",
    "                if name == k:\n",
    "                    image_index = i\n",
    "                    break\n",
    "            img2dna_indices[image_index] = dna_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c365511-51bc-4ae4-ba22-9965e060b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2dna_indices\n",
    "train_indices\n",
    "\n",
    "dna_features2 = []\n",
    "dna_labels2 = []\n",
    "for i in train_indices:\n",
    "    dna_features2.append(train_dna_features[img2dna_indices[i]])\n",
    "    dna_labels2.append(train_dna_labels[img2dna_indices[i]])\n",
    "expanded_train_dna_features = torch.stack(dna_features2)\n",
    "expanded_train_dna_labels = torch.stack(dna_labels2)\n",
    "\n",
    "dna_features2 = []\n",
    "dna_labels2 = []\n",
    "for i in val_indices:\n",
    "    dna_features2.append(val_dna_features[img2dna_indices[i]])\n",
    "    dna_labels2.append(val_dna_labels[img2dna_indices[i]])\n",
    "expanded_val_dna_features = torch.stack(dna_features2)\n",
    "expanded_val_dna_labels = torch.stack(dna_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "407d99bb-49b2-45d6-96fe-4658d36ee861",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(expanded_train_dna_features,'features/deeper_expanded_train_dna_features.pt')\n",
    "torch.save(expanded_val_dna_features,'features/deeper_expanded_val_dna_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19557b0e-06a6-47be-90c6-6a13f0d75b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(expanded_train_dna_labels,'features/deeper_expanded_train_dna_labels.pt')\n",
    "torch.save(expanded_val_dna_labels,'features/deeper_expanded_val_dna_labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ba2fe-fdd4-4f87-9e7c-3e60aff43567",
   "metadata": {},
   "source": [
    "# Paper approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612cc7b5-c064-46a5-97c8-8bd818fa128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNA_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNA_CNN, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(3, 1), stride=(3, 1))\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(3, 1), stride=(3, 1))\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(5840, 1000)  # Adjust the input size of this layer based on the output size after convolutions and flattening\n",
    "        self.dropout_fc1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1000, 1050)  # Assuming 10 classes for the final output; adjust accordingly\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout_fc1(x)\n",
    "        x = self.fc2(x)  # Apply softmax during loss computation (e.g., using nn.CrossEntropyLoss)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def feature_extract(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        #x = self.fc1(x)\n",
    "        #x = torch.tanh(x)\n",
    "        #x = self.fc2(x)  # Apply softmax during loss computation (e.g., using nn.CrossEntropyLoss)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "174ca6bd-4d08-4fe1-b0a4-df8d72cad5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98286b49dbe489e8eef13a0ef3f9735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa32e287c3a467389e4a26e0fec7eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], train_loss: 0.6543,  train_score: 0.3801,val_loss: 3.0349,  val_score: 0.4455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e191d0df2e074ac98e5ba2394f657a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f26844d96ff4cfcbb0e26747348d559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], train_loss: 0.0709,  train_score: 0.9548,val_loss: 5.3085,  val_score: 0.4751\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb1bdcf13fb4c28bd0deaefa9f94f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f8760edda441d1aafac63f0be90449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], train_loss: 0.0222,  train_score: 0.9889,val_loss: 4.9694,  val_score: 0.4748\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ec2285a0ec48ffa8e5eadf7c369135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a078a140c95746d09a90e3f3757b6605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], train_loss: 0.0464,  train_score: 0.9957,val_loss: 7.2141,  val_score: 0.4751\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67113a137f8547419d6a1ae38cfa55fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed59703daae44f79e827770d3161307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], train_loss: 0.0264,  train_score: 0.9968,val_loss: 7.5824,  val_score: 0.4760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcb83bf5ad2446faf68aa6c8e0c24f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aac1a32c4e04d718789304734c1c6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], train_loss: 0.0059,  train_score: 0.9950,val_loss: 9.1347,  val_score: 0.4756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39230816d58141d2928d019e8ad2fa67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a85d5433d054f028124d38d74e7df6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], train_loss: 0.0130,  train_score: 0.9955,val_loss: 6.2327,  val_score: 0.4753\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdabe8b5bd904e88aa4ea1167ab06a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03cfff6b7094a8abf945e70d46cc4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], train_loss: 0.0078,  train_score: 0.9961,val_loss: 7.8002,  val_score: 0.4751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0.6543, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0709, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0222, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0464, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0264, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0059, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0130, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.0078, grad_fn=<NllLossBackward0>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnamodel = DNA_CNN()\n",
    " \n",
    "optimizer = torch.optim.Adam(dnamodel.parameters(),weight_decay=1e-5)\n",
    "\n",
    "fit(8,dataloaders,optimizer,dnamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f735729-313c-43ed-b393-ab785eca49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(d_train, batch_size=len(d_train),shuffle=False)\n",
    "dataloader_val = DataLoader(d_val, batch_size=len(d_val),shuffle=False)\n",
    "dataloaders = {'train':dataloader_train,'val':dataloader_val}\n",
    "dataset_sizes = {'train': d_train.data.shape[0], 'val':d_val.data.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d88890-03b1-44fb-9085-54415bdfca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10840, 5840])\n"
     ]
    }
   ],
   "source": [
    "dnamodel.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "\n",
    "    for dnas,labels in dataloaders['train']:\n",
    "        dnas = dnas.to(device)\n",
    "        train_dna_features = dnamodel.feature_extract(dnas)\n",
    "        train_dna_labels = labels\n",
    "    for dnas,labels in dataloaders['val']:\n",
    "        dnas = dnas.to(device)\n",
    "        val_dna_features = dnamodel.feature_extract(dnas)\n",
    "        val_dna_labels = labels\n",
    "print(train_dna_features.shape)\n",
    "\n",
    "train_dna_features = train_dna_features.cpu()\n",
    "val_dna_features = val_dna_features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "138bfcc6-d923-4cfd-8325-cd2b3b306b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2dna_indices = dict()\n",
    "for k,v in img2dna.items():\n",
    "    dna_index = np.where(X_train['processid'].values == v)\n",
    "    if dna_index[0].size > 0:\n",
    "        dna_index = dna_index[0][0]\n",
    "        \n",
    "        for i,(name,_) in enumerate(image_dataset.imgs):\n",
    "            if name == k:\n",
    "                image_index = i\n",
    "                break\n",
    "        img2dna_indices[image_index] = dna_index\n",
    "    else:\n",
    "        dna_index = np.where(X_validation['processid'].values == v)\n",
    "        if dna_index[0].size > 0:\n",
    "            dna_index = dna_index[0][0]\n",
    "            for i,(name,_) in enumerate(image_dataset.imgs):\n",
    "                if name == k:\n",
    "                    image_index = i\n",
    "                    break\n",
    "            img2dna_indices[image_index] = dna_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b14c679-591e-40da-a408-6ba0647f382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2dna_indices\n",
    "train_indices\n",
    "\n",
    "dna_features2 = []\n",
    "dna_labels2 = []\n",
    "for i in train_indices:\n",
    "    dna_features2.append(train_dna_features[img2dna_indices[i]])\n",
    "    dna_labels2.append(train_dna_labels[img2dna_indices[i]])\n",
    "expanded_train_dna_features = torch.stack(dna_features2)\n",
    "expanded_train_dna_labels = torch.stack(dna_labels2)\n",
    "\n",
    "dna_features2 = []\n",
    "dna_labels2 = []\n",
    "for i in val_indices:\n",
    "    dna_features2.append(val_dna_features[img2dna_indices[i]])\n",
    "    dna_labels2.append(val_dna_labels[img2dna_indices[i]])\n",
    "expanded_val_dna_features = torch.stack(dna_features2)\n",
    "expanded_val_dna_labels = torch.stack(dna_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1eeca0c-f101-43d9-91bf-8e502b47b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(expanded_train_dna_features,'features/dnamodel_expanded_train_dna_features.pt')\n",
    "torch.save(expanded_val_dna_features,'features/dnamodel_expanded_val_dna_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a01810d-f215-4803-b1e2-2ca31289c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(expanded_train_dna_labels,'features/dnamodel_expanded_train_dna_labels.pt')\n",
    "torch.save(expanded_val_dna_labels,'features/dnamodel_expanded_val_dna_labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61451441-228d-4ba1-961a-4f1190cd620e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Transformer approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e40a3490-244d-45fb-8cba-bc63363cc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, nhead, num_encoder_layers, dim_feedforward, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, (5, 1), stride=(2, 1), padding=(2, 0))\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d((2, 1))  # Pooling layer to reduce dimensionality\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, (5, 1), stride=(2, 1), padding=(2, 0))\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1))  # Pooling layer to reduce dimensionality\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, (5, 1), stride=(2, 1), padding=(2, 0))\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d((2, 1))  # Pooling layer to reduce dimensionality\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # Example input size, to be calculated dynamically\n",
    "        example_input = torch.zeros(1, 1, 30, 1)  # Adjust the dimensions according to your input\n",
    "        self.conv_output_size = self._get_conv_output_size(example_input)\n",
    "        \n",
    "        self.input_dim = self.conv_output_size\n",
    "        if self.input_dim % nhead != 0:\n",
    "            raise ValueError(f\"input_dim ({self.input_dim}) must be divisible by nhead ({nhead})\")\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.input_dim, nhead=nhead, dim_feedforward=dim_feedforward\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.input_dim, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.activation4 = nn.LeakyReLU()\n",
    "        \n",
    "        self.linear2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _get_conv_output_size(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        \n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        \n",
    "        # Reshape for transformer: (seq_length, batch_size, input_dim)\n",
    "        x = x.view(1, x.size(0), -1)\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        x = x.view(x.size(1), -1)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.activation4(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e01c6ab1-cff1-42ef-8e91-b03f2912bfdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 64, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m dim_feedforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      4\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1050\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerModel(nhead, num_encoder_layers, dim_feedforward, num_classes)\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(hybridmodel\u001b[38;5;241m.\u001b[39mparameters(),weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     10\u001b[0m fit(\u001b[38;5;241m40\u001b[39m,dataloaders,optimizer,model)\n",
      "Cell \u001b[0;32mIn[25], line 24\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[0;34m(self, nhead, num_encoder_layers, dim_feedforward, num_classes)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Example input size, to be calculated dynamically\u001b[39;00m\n\u001b[1;32m     23\u001b[0m example_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust the dimensions according to your input\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_conv_output_size(example_input)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_output_size\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m%\u001b[39m nhead \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[25], line 56\u001b[0m, in \u001b[0;36mTransformerModel._get_conv_output_size\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation3(x)\n\u001b[0;32m---> 56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x)\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool3(x)\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/insects/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/insects/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/insects/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    184\u001b[0m     bn_training,\n\u001b[1;32m    185\u001b[0m     exponential_average_factor,\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    187\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/insects/lib/python3.12/site-packages/torch/nn/functional.py:2480\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2468\u001b[0m         batch_norm,\n\u001b[1;32m   2469\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2477\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2478\u001b[0m     )\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2480\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2484\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/insects/lib/python3.12/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2446\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 64, 1, 1])"
     ]
    }
   ],
   "source": [
    "nhead = 8  # Adjusting nhead to a divisor of the new calculated input_dim\n",
    "num_encoder_layers = 2\n",
    "dim_feedforward = 512\n",
    "num_classes = 1050\n",
    "\n",
    "model = TransformerModel(nhead, num_encoder_layers, dim_feedforward, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(hybridmodel.parameters(),weight_decay=1e-4)\n",
    "\n",
    "fit(40,dataloaders,optimizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63eda910-9705-487f-9395-4981718899e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(d_train, batch_size=len(d_train),shuffle=False)\n",
    "dataloader_val = DataLoader(d_val, batch_size=len(d_val),shuffle=False)\n",
    "dataloaders = {'train':dataloader_train,'val':dataloader_val}\n",
    "dataset_sizes = {'train': d_train.data.shape[0], 'val':d_val.data.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b78c017-2052-4215-87a3-a86a20020033",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hybrid_CNN_LSTM' object has no attribute 'feature_extract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dnas,labels \u001b[38;5;129;01min\u001b[39;00m dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      6\u001b[0m     dnas \u001b[38;5;241m=\u001b[39m dnas\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     train_dna_features \u001b[38;5;241m=\u001b[39m hybridmodel\u001b[38;5;241m.\u001b[39mfeature_extract(dnas)\n\u001b[1;32m      8\u001b[0m     train_dna_labels \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dnas,labels \u001b[38;5;129;01min\u001b[39;00m dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/insects/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Hybrid_CNN_LSTM' object has no attribute 'feature_extract'"
     ]
    }
   ],
   "source": [
    "hybridmodel.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "\n",
    "    for dnas,labels in dataloaders['train']:\n",
    "        dnas = dnas.to(device)\n",
    "        train_dna_features = hybridmodel.feature_extract(dnas)\n",
    "        train_dna_labels = labels\n",
    "    for dnas,labels in dataloaders['val']:\n",
    "        dnas = dnas.to(device)\n",
    "        val_dna_features = hybridmodel.feature_extract(dnas)\n",
    "        val_dna_labels = labels\n",
    "print(train_dna_features.shape)\n",
    "\n",
    "train_dna_features = train_dna_features.cpu()\n",
    "val_dna_features = val_dna_features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc663a-d48b-416b-8626-51bdc9e5ccb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

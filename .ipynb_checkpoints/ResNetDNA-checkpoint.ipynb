{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d221a0d-06ce-4ae1-a19e-4eaba01167d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = torchvision.datasets.ImageFolder(\"image_dataset/\")\n",
    "df = pd.read_csv('final_dataset.csv',index_col=0)\n",
    "df = df[df['species_name']!= 'Agabus sturmii']\n",
    "df['species_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9598e1-9d9a-43ed-9b6f-0f83431da524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(nucleotide: str, seq_len=658) -> np.ndarray:\n",
    "    # Cutting the sequence if it is longer than a pre-defined value seq_len\n",
    "    if len(nucleotide) > seq_len:\n",
    "        nucleotide = nucleotide[:seq_len]\n",
    "    # Encoding\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    sequence = [mapping[i] if i in mapping else 4 for i in nucleotide]\n",
    "    encoded_sequence = np.eye(5)[sequence]\n",
    "    # Padding if the sequence is smaller than a pre-defined value seq_len\n",
    "    if len(encoded_sequence) < seq_len:\n",
    "        padding = np.zeros((seq_len - len(encoded_sequence), 5))\n",
    "        encoded_sequence = np.concatenate((encoded_sequence, padding))\n",
    "    \n",
    "    return encoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7bd1e5-2be6-4ebc-b5fa-4a3c4654779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotides = df[['nucleotide','species_name','genus_name']]\n",
    "colonna_dna = df.loc[:,\"nucleotide\"]\n",
    "nucleotides.loc[:,'nucleotide'] = colonna_dna.apply(one_hot_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744f596f-412b-4d68-a5d6-c3f5b244aee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            nucleotide  \\\n",
      "0    [[0.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0....   \n",
      "1    [[1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0....   \n",
      "2    [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0....   \n",
      "3    [[0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0....   \n",
      "4    [[0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0....   \n",
      "..                                                 ...   \n",
      "418  [[1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0....   \n",
      "419  [[1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0....   \n",
      "420  [[1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0....   \n",
      "421  [[1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0....   \n",
      "423  [[0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0....   \n",
      "\n",
      "               species_name  genus_name  \n",
      "0         Leucania cruegeri    Leucania  \n",
      "1             Lestica alata     Lestica  \n",
      "2    Liotryphon punctulatus  Liotryphon  \n",
      "3        Lesmone formularis     Lesmone  \n",
      "4        Lesmone formularis     Lesmone  \n",
      "..                      ...         ...  \n",
      "418          Hemiceras losa   Hemiceras  \n",
      "419    Hemiceras nigrescens   Hemiceras  \n",
      "420    Hemiceras nigrescens   Hemiceras  \n",
      "421      Hemiceras punctata   Hemiceras  \n",
      "423    Hemiceras nigrescens   Hemiceras  \n",
      "\n",
      "[26463 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(nucleotides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2e2194-56aa-4576-89e6-f487e2393b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, test_ratio):\n",
    "    test = []\n",
    "    genus_count = df.groupby('genus_name')['species_name'].nunique()\n",
    "    \n",
    "    for genus_name in genus_count.index:\n",
    "        number_undescribed_species = genus_count[genus_name]//3\n",
    "        species = list(df.loc[df['genus_name']==genus_name]['species_name'].unique())\n",
    "        undescribed_species = random.sample(species,number_undescribed_species)\n",
    "        test = test+undescribed_species\n",
    "\n",
    "    df_remaining = df.loc[~df.species_name.isin(test)]\n",
    "    df_undescribed = df.loc[df.species_name.isin(test)]\n",
    "    \n",
    "    y = df_remaining['species_name']\n",
    "    X = df_remaining.drop(columns=['species_name'])\n",
    "    \n",
    "    y_undescribed = df_undescribed['species_name']\n",
    "    X_undescribed = df_undescribed.drop(columns=['species_name'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
    "    \n",
    "    y_test = pd.concat([y_test,y_undescribed])\n",
    "    X_test = pd.concat([X_test,X_undescribed])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "781c8bc4-0dc2-4422-8282-d59e30a955b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "X_train_1, X_test, y_train_1, y_test = data_split(nucleotides,0.3)\n",
    "\n",
    "train_data = X_train_1\n",
    "train_data['species_name'] = y_train_1\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = data_split(train_data,0.2)\n",
    "X_train = X_train.drop(columns=['genus_name'])\n",
    "X_validation= X_validation.drop(columns=['genus_name'])\n",
    "X_test = X_test.drop(columns=['genus_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143ed9a1-8f54-4deb-bf5e-2a23fd7cb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.apply(lambda x: dataset.class_to_idx[x.replace(' ','_')])\n",
    "y_test = y_test.apply(lambda x: dataset.class_to_idx[x.replace(' ','_')])\n",
    "y_validation= y_validation.apply(lambda x: dataset.class_to_idx[x.replace(' ','_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d37a1-d595-43f5-b17c-294f2b593fb2",
   "metadata": {},
   "source": [
    "# ResNet DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8edb15f-bcac-45fd-9aad-1f2ce3eb9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DNAdataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.tensor(targets)\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(np.float32(self.data[index][0]))\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        #if self.transform:\n",
    "        #    x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "        #    x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c86e9ce1-ef6e-4bb4-ae7e-e02c770c11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = DNAdataset(X_train.values, y_train.values)\n",
    "d_val = DNAdataset(X_validation.values, y_validation.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaca31c-ada7-4387-a1a3-72822846659b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0bcf32-4723-4407-acc6-07bbca3d2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(d_train, batch_size=1)\n",
    "dataloader_val = DataLoader(d_val, batch_size=1)\n",
    "dataloaders = {'train':dataloader_train,'val':dataloader_val}\n",
    "dataset_sizes = {'train': d_train.data.shape[0], 'val':d_val.data.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92101036-aae6-43fb-943f-ed35dd5483f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " #inputs, classes = next(iter(dataloader))   \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "348f5ee6-051d-4341-bcd0-06e484657ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    #print(inputs.shape)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aa250c9-3377-433c-92b2-074c138a8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "#        self.linear1 = torch.nn.Linear(658, 200)\n",
    "        self.conv1 = torch.nn.Conv2d(1,5,3)\n",
    "        self.activation = torch.nn.LeakyReLU()\n",
    "        #self.conv2 = torch.nn.Conv2d(2, 2,1)\n",
    "        self.conv2 = torch.nn.Conv2d(5,1,(5,1))\n",
    "        self.activation2 = torch.nn.LeakyReLU()\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.linear = torch.nn.Linear(652*3,1049)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "tinymodel.cuda()\n",
    "optimizer = torch.optim.SGD(tinymodel.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr=0.01,epochs= 25, steps_per_epoch= 10) \n",
    "#torch.optim.lr_scheduler.StepLR(optimizer,10)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd036ed-01fe-4431-90d7-31a2f6755780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fogliodicarta/miniconda3/envs/insetti/lib/python3.12/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 6.9556 Acc: 0.0020\n",
      "val Loss: 6.9556 Acc: 0.0004\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 6.9556 Acc: 0.0022\n",
      "val Loss: 6.9556 Acc: 0.0004\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 6.9556 Acc: 0.0024\n",
      "val Loss: 6.9556 Acc: 0.0006\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 6.9556 Acc: 0.0024\n",
      "val Loss: 6.9556 Acc: 0.0008\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 6.9556 Acc: 0.0024\n",
      "val Loss: 6.9556 Acc: 0.0008\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 6.9555 Acc: 0.0024\n",
      "val Loss: 6.9556 Acc: 0.0008\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 6.9555 Acc: 0.0044\n",
      "val Loss: 6.9556 Acc: 0.0083\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 6.9538 Acc: 0.0181\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 6.9435 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 6.9435 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 6.9435 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 6.9435 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 6.9435 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 6.9435 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 6.9436 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 6.9435 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 6.9435 Acc: 0.0137\n",
      "val Loss: 6.9505 Acc: 0.0067\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(tinymodel,torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),optimizer,scheduler)\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/insetti/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/insetti/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(tinymodel,torch.nn.CrossEntropyLoss(),optimizer,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0dd861-2c26-48b2-b754-c92e68e27e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36c85f-1298-48d7-95f0-fec78560f827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

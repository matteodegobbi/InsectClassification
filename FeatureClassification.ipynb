{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b94cb3-7047-49d6-b2d6-937692c7d536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataset_utils' from '/home/fogliodicarta/Desktop/InsectClassification/dataset_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image \n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import dataset_utils\n",
    "import importlib\n",
    "importlib.reload(dataset_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29950691-4341-4247-a273-43e53ed45583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (embed): Embedding(1050, 1000)\n",
       "  (main): Sequential(\n",
       "    (0): Tpose(\n",
       "      (main): Sequential(\n",
       "        (0): ConvTranspose2d(1100, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Tpose(\n",
       "      (main): Sequential(\n",
       "        (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Tpose(\n",
       "      (main): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Tpose(\n",
       "      (main): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Tpose(\n",
       "      (main): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modelC as m\n",
    "def extract_features(self,x):\n",
    "    x = self.main(x)\n",
    "    x = self.flatten(x)\n",
    "    return x\n",
    "m.Discriminator.extract_features = extract_features\n",
    "\n",
    "discriminator = m.Discriminator()\n",
    "discriminator.cuda()\n",
    "generator = m.Generator(noise_size=100)\n",
    "generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e169b430-c943-4102-a623-f776f68c2778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_dict = torch.load('checkpoints/generatorC')\n",
    "generator.load_state_dict(g_dict['model_state_dict'])\n",
    "d_dict = torch.load('checkpoints/discriminatorC')\n",
    "discriminator.load_state_dict(d_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22757b9b-7690-4912-bc5a-f2d2b27a1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_dataset.csv',index_col=0)\n",
    "tform = transforms.Compose([transforms.Resize((64,64)),transforms.PILToTensor(),transforms.ConvertImageDtype(torch.float),transforms.Normalize(0.5,0.5)])\n",
    "image_dataset = torchvision.datasets.ImageFolder(\"image_dataset/\",transform=tform)\n",
    "species2genus = dataset_utils.species_label_to_genus_label(df,image_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb75374-8087-4540-8210-6478414c3cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365    Bembidion normannum\n",
      "292       Bledius gallicus\n",
      "321       Praxis edwardsii\n",
      "352        Andrena pilipes\n",
      "18     Automeris managuana\n",
      "              ...         \n",
      "412         Hemiceras losa\n",
      "413         Hemiceras losa\n",
      "417     Hemiceras punctata\n",
      "418         Hemiceras losa\n",
      "421     Hemiceras punctata\n",
      "Name: species_name, Length: 9991, dtype: object\n",
      "32424\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000 \n",
    "import random\n",
    "import dataset_utils\n",
    "img2dna = dataset_utils.get_imgs_bold_id(image_dataset,df)\n",
    "\n",
    "nucleotides = df[['nucleotide','species_name','genus_name','processid','image_urls']]\n",
    "colonna_dna = df.loc[:,\"nucleotide\"]\n",
    "nucleotides.loc[:,'nucleotide'] = colonna_dna.apply(dataset_utils.one_hot_encoding)\n",
    "random.seed(42)\n",
    "\n",
    "X_train_1, X_test, y_train_1, y_test = dataset_utils.data_split(nucleotides,0.2,random_state=42)\n",
    "print(y_test)\n",
    "train_data = X_train_1\n",
    "train_data['species_name'] = y_train_1\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = dataset_utils.data_split(train_data,0.2,drop_labels=False,random_state=42)\n",
    "train_indices, val_indices, test_indices = dataset_utils.image_splits_from_df(X_train,X_validation,X_test,image_dataset)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class WholeDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = data.targets#torch.tensor(targets)\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index][0]\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "whole_dataset = WholeDataset(image_dataset)\n",
    "n_classes = np.unique(whole_dataset.targets).shape[0]\n",
    "print(len(test_indices+val_indices+train_indices))\n",
    "len(whole_dataset)\n",
    "\n",
    "train_imgs = torch.utils.data.Subset(whole_dataset, train_indices)\n",
    "val_imgs = torch.utils.data.Subset(whole_dataset, val_indices)\n",
    "test_imgs = torch.utils.data.Subset(whole_dataset, test_indices)\n",
    "len(train_imgs)+len(val_imgs)+len(test_imgs)\n",
    "train_loader = torch.utils.data.DataLoader(train_imgs, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_imgs, batch_size=2000,shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_imgs, batch_size=4,shuffle=True, num_workers=2)\n",
    "del test_loader\n",
    "dataloaders = {\"train\":train_loader,\"val\":val_loader}\n",
    "dataset_sizes = {'train': len(train_imgs.indices), 'val':len(val_imgs.indices)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2094cad-d26d-4bf5-ba8d-866f31c0e33c",
   "metadata": {},
   "source": [
    "## Extract features from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faa6ece7-7018-449f-a7b3-5f32fb8bd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.eval()\n",
    "train_features = []\n",
    "train_labels = np.array([]) \n",
    "with torch.no_grad():\n",
    "    for batch, targets in dataloaders['train']:\n",
    "        features_torch = discriminator.extract_features(batch.cuda()) \n",
    "        features_targets_torch = targets\n",
    "        train_labels = np.concatenate((train_labels, features_targets_torch.cpu().numpy()))\n",
    "        train_features.append(features_torch.cpu().numpy())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "train_features = np.concatenate(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7f67bc-bc04-4030-9c90-4e35413e24c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13039,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape\n",
    "train_labels.shape\n",
    "#train_labels= features_targets.cpu().numpy()\n",
    "#train_features = features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492aa5cc-3d16-4e82-a862-550d199f8a94",
   "metadata": {},
   "source": [
    "## Train Random Forest on training set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d251111-5b29-4eb6-8e36-40468d82f23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "#svm = LinearSVC(random_state=42)\n",
    "\n",
    "# Make it an Multilabel classifier\n",
    "#multilabel_classifier = MultiOutputClassifier(svm, n_jobs=-1)\n",
    "\n",
    "# Fit the data to the Multilabel classifier\n",
    "#svm = svm.fit(train_features, train_labels)\n",
    "\n",
    "# Get predictions for test data\n",
    "#y_test_pred = svm.predict(train_features)\n",
    "clf = RandomForestClassifier(min_samples_leaf=2,n_jobs=-1)\n",
    "clf = clf.fit(train_features, train_labels)\n",
    "#n_estimators = 1\n",
    "#clf = (BaggingClassifier(RandomForestClassifier(min_samples_leaf=50), max_samples=1.0 / n_estimators, n_estimators=n_estimators,n_jobs=10))\n",
    "#clf = clf.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2bd1db3-fb83-4bdc-87d8-14168129cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training species accuracy:0.9996932280082829\n"
     ]
    }
   ],
   "source": [
    "train_predicted_labels = clf.predict(train_features)\n",
    "print(f\"Training species accuracy:{np.count_nonzero(train_predicted_labels==train_labels)/len(train_labels)}\")\n",
    "\n",
    "\n",
    "#np.unique(train_predicted_labels.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f0beb-27a2-4ab1-952f-943d74518fa5",
   "metadata": {},
   "source": [
    "## Extract features from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f580bfdc-2272-45ab-af5e-d70c66ad96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "821b0267-a561-40ad-a324-74e676f3c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.eval()\n",
    "val_features = []\n",
    "val_labels = np.array([])\n",
    "with torch.no_grad():\n",
    "    for batch,targets in dataloaders['val']:\n",
    "        features_torch = discriminator.extract_features(batch.cuda()) \n",
    "        features_targets_torch = targets\n",
    "        val_labels = np.concatenate((val_labels, features_targets_torch.cpu().numpy()))\n",
    "        val_features.append(features_torch.cpu().numpy())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "val_features = np.concatenate(val_features)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "479095da-2afe-4f5f-aced-c3091cb50c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation species accuracy:0.20862688713156002\n"
     ]
    }
   ],
   "source": [
    "val_predicted_labels= clf.predict(val_features)\n",
    "print(f\"Validation species accuracy:{np.count_nonzero(val_predicted_labels==val_labels)/len(val_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "620d60d1-e5a6-4189-a4c8-168420b5b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_val_predicted_probs = clf.predict_proba(val_features)\n",
    "val_predicted_probs = np.zeros((len(temp_val_predicted_probs),n_classes))\n",
    "for i, cls in enumerate(np.arange(n_classes)):\n",
    "    if cls in clf.classes_:\n",
    "        val_predicted_probs[:, cls] = temp_val_predicted_probs[:, list(clf.classes_).index(cls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b9caa92-b16f-4ef1-9356-107aa0b242bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding zeroes to classes not in training set:  (6955, 703)\n",
      "After adding zeroes:  (6955, 1050)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before adding zeroes to classes not in training set: \",temp_val_predicted_probs.shape)\n",
    "print(\"After adding zeroes: \",val_predicted_probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfac6d2-3f70-4b95-bace-56119972adcc",
   "metadata": {},
   "source": [
    "## Computing genus accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04cf5073-b9c8-403c-ab0a-7516f8c06985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation genus accuracy: 0.5072609633357297\n"
     ]
    }
   ],
   "source": [
    "#i = 100\n",
    "import math\n",
    "n_correct_genus = 0\n",
    "for i in range(len(val_labels)):\n",
    "    #label_best_specie = val_predicted_probs[i].argmax()\n",
    "    label_best_specie = val_predicted_labels[i]\n",
    "    assert(val_predicted_labels[i]==val_predicted_probs[i].argmax())\n",
    "    genus_of_best_species = species2genus[label_best_specie.item()]\n",
    "    #species_same_genus = [k for k,v in species2genus.items() if v == genus_of_best_species]\n",
    "    #reduced_species = val_predicted_probs[i][species_same_genus]\n",
    "    #normalized_reduced_species = reduced_species/(reduced_species.sum())\n",
    "    \n",
    "    real_genus = species2genus[val_labels[i]]\n",
    "    predicted_genus = genus_of_best_species\n",
    "    if real_genus == predicted_genus:\n",
    "        n_correct_genus+=1\n",
    "print(f\"Validation genus accuracy: {n_correct_genus/len(val_labels)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bce4f8b-fc20-4cca-a13f-8146d9a6a81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training genus accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "n_correct_genus = 0\n",
    "for i in range(len(train_labels)):\n",
    "    #label_best_specie = val_predicted_probs[i].argmax()\n",
    "    label_best_specie = train_predicted_labels[i]\n",
    "    genus_of_best_species = species2genus[label_best_specie.item()]\n",
    "    #species_same_genus = [k for k,v in species2genus.items() if v == genus_of_best_species]\n",
    "    #reduced_species = val_predicted_probs[i][species_same_genus]\n",
    "    #normalized_reduced_species = reduced_species/(reduced_species.sum())\n",
    "    \n",
    "    real_genus = species2genus[train_labels[i]]\n",
    "    predicted_genus = genus_of_best_species\n",
    "    if real_genus == predicted_genus:\n",
    "        n_correct_genus+=1\n",
    "print(f\"Training genus accuracy: {n_correct_genus/len(train_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198040a-0470-43c6-8b59-031e370d637f",
   "metadata": {},
   "source": [
    "## Compute wether we output genus or species based on average entropy per symbol $H_S$ of the species belonging to genus of the species with the highest likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94f131af-064a-4aff-aa91-96a8dd06c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_per_symbol(v):\n",
    "    entropy = 0 \n",
    "    for p in v:\n",
    "        entropy-=p*math.log(p,2)\n",
    "    mean_symbol_entropy = entropy/math.log(len(normalized_reduced_species),2)\n",
    "    return mean_symbol_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eff834-1ce3-46e1-9395-e1aa35d4837d",
   "metadata": {},
   "source": [
    "# COPIA DALLA CELLA SOPRA PER OTTENERE LE SPECIE DEL GENUS CORRISPONDENTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259fd04-2038-450f-8003-88f11d6b6092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
